{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import sparse\n",
    "\n",
    "import heapq\n",
    "import itertools\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "NORMED = 0\n",
    "SVD = 1\n",
    "\n",
    "s_words = set(stopwords.words(\"english\"))\n",
    "articles_path = \"resources/articles\"\n",
    "index_map_path = \"resources/map.txt\"\n",
    "u_path = \"resources/svd/u.npy\"\n",
    "dvt_path = \"resources/svd/dvt.npy\"\n",
    "idf_path = \"resources/svd/idf.npy\"\n",
    "terms_matrix_path = \"resources/sparse/terms_matrix.npz\"\n",
    "paths = [\"resources/svd\", \"resources/sparse\"]\n",
    "\n",
    "\n",
    "def get_all_txt(path):\n",
    "    list_of_files = []\n",
    "    for r, _, files in os.walk(path):\n",
    "        for f in files:\n",
    "            if f.endswith(\".txt\") and os.path.getsize(os.path.join(r,f)) > 0: \n",
    "                list_of_files.append(os.path.join(r,f))\n",
    "    return list_of_files\n",
    "\n",
    "    \n",
    "def article_reader(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            for word in line.split(\" \"):\n",
    "                if word and word not in s_words and not word.isnumeric():\n",
    "                    yield word\n",
    "\n",
    "                    \n",
    "def word_trim(word):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(word)\n",
    "    \n",
    "\n",
    "class SearchEngine:\n",
    "    def __init__(self, preprocess=False, k=10):\n",
    "        self.filenames = get_all_txt(articles_path)[:100]\n",
    "        self.files_count = len(self.filenames)\n",
    "            \n",
    "        self.word_map = {}\n",
    "        self.index_map = None\n",
    "        self.bag_of_words = None\n",
    "        self.terms_matrix = None\n",
    "        self.idf_m = None\n",
    "\n",
    "        self.current_k = k\n",
    "        \n",
    "        self.u = None\n",
    "        self.dvt = None\n",
    "        \n",
    "        if preprocess:\n",
    "            if not os.path.isdir(\"resources/articles\"):\n",
    "                raise(\"Cannot run engine without ./resources/articles\")\n",
    "            for p in paths:\n",
    "                if not os.path.isdir(p):\n",
    "                    os.mkdir(p)\n",
    "            self.compute_bag_of_words()\n",
    "            self.create_mappings()\n",
    "            open(index_map_path, \"w\").writelines(f\"{word}\\n\" for word in self.index_map)\n",
    "            self.words_count = len(self.index_map)\n",
    "            \n",
    "            self.compute_terms_matrix()\n",
    "            sparse.save_npz(terms_matrix_path, self.terms_matrix)\n",
    "            np.save(idf_path, self.idf_m)\n",
    "            \n",
    "            self.compute_svd()\n",
    "            np.save(u_path, self.u)\n",
    "            np.save(dvt_path, self.dvt)\n",
    "        \n",
    "        else:\n",
    "            self.u = np.load(u_path)\n",
    "            self.dvt = np.load(dvt_path)\n",
    "            self.idf_m = np.load(idf_path)\n",
    "            self.index_map = [word.rstrip() for word in open(index_map_path, \"r\").readlines()]\n",
    "            for (i, word) in enumerate(self.index_map):\n",
    "                self.word_map[word] = i\n",
    "            self.terms_matrix = sparse.load_npz(terms_matrix_path)\n",
    "            self.words_count = len(self.index_map)\n",
    "            print(self.words_count)\n",
    "    \n",
    "    \n",
    "    def compute_bag_of_words(self):\n",
    "        self.bag_of_words = set()\n",
    "        for file in self.filenames:\n",
    "            for word in article_reader(file):\n",
    "                self.bag_of_words.add(word_trim(word))\n",
    "\n",
    "\n",
    "    def create_mappings(self):\n",
    "        self.index_map = []\n",
    "        for (i, word) in enumerate(self.bag_of_words):\n",
    "            self.index_map.append(word)\n",
    "            self.word_map[word] = i\n",
    "        print(f\"Words quantity: {len(self.index_map)}\")\n",
    "        print(\"__________________________\")\n",
    "\n",
    "\n",
    "    def compute_terms_matrix(self):\n",
    "        tmp_matrix = sparse.lil_matrix((self.words_count, self.files_count), dtype=np.float32)\n",
    "        print(\"Terms matrix computing...\")\n",
    "        for i, file_name in enumerate(self.filenames):\n",
    "            for word in article_reader(file_name):\n",
    "                tmp_matrix[self.word_map[word_trim(word)], i] += 1\n",
    "        \n",
    "        self.terms_matrix = tmp_matrix.tocsr()\n",
    "        print(\"Done.\\n__________________________\")\n",
    "        \n",
    "        print(\"IDF formatting...\")\n",
    "        self.idf()\n",
    "        self.idf_matrix_format()\n",
    "        print(\"Done.\\n__________________________\")\n",
    "        \n",
    "        print(\"Terms matrix normalizing...\")\n",
    "        d_norms = sparse.linalg.norm(self.terms_matrix, axis=0)\n",
    "        \n",
    "        non_zero_tmp = self.terms_matrix.nonzero()\n",
    "        non_zero = itertools.zip_longest(non_zero_tmp[0], non_zero_tmp[1])\n",
    "        for (row, col) in non_zero:\n",
    "            self.terms_matrix[row, col] = self.terms_matrix[row, col] / d_norms[col]\n",
    "        print(\"Done.\\n__________________________\")\n",
    "        \n",
    "        \n",
    "    def compute_svd(self):\n",
    "        print(\"Computing SVD decomposition...\")\n",
    "        self.u, d, vt = sparse.linalg.svds(self.terms_matrix,k=self.current_k)\n",
    "        print(\"Done.\\n__________________________\")\n",
    "        \n",
    "        print(\"Computing D @ V.T...\")\n",
    "        self.dvt = sparse.diags(d).dot(vt)\n",
    "        print(\"Done.\\n__________________________\")\n",
    "        \n",
    "        print(\"Preparing D @ V.T to give normalized Ak matrix...\")\n",
    "        for col in range(self.files_count):\n",
    "            norm = np.linalg.norm(self.u @ self.dvt[:,col])\n",
    "            self.dvt[:,col] /= norm\n",
    "        print(\"Done.\\n__________________________\")\n",
    "\n",
    "\n",
    "    def words_frequency(self, words):\n",
    "        result = sparse.lil_matrix((1, self.words_count), dtype=np.float32)\n",
    "        for word in words:\n",
    "            trimmed = word_trim(word)\n",
    "            if trimmed in self.word_map:\n",
    "                result[0, self.word_map[trimmed]] += 1\n",
    "        return result.tocsr()\n",
    "\n",
    "\n",
    "    def idf(self):\n",
    "        self.idf_m = np.zeros(self.words_count, dtype=np.float32)\n",
    "        \n",
    "        non_zero_tmp = self.terms_matrix.nonzero()\n",
    "        non_zero = itertools.zip_longest(non_zero_tmp[0], non_zero_tmp[1])\n",
    "        for (row_n, col_n) in non_zero:\n",
    "            self.idf_m[row_n] += 1\n",
    "        \n",
    "        for row_n in range(self.words_count):\n",
    "            self.idf_m[row_n] = np.log(self.files_count / self.idf_m[row_n])\n",
    "\n",
    "\n",
    "    def idf_matrix_format(self):\n",
    "        non_zero_tmp = self.terms_matrix.nonzero()\n",
    "        non_zero = itertools.zip_longest(non_zero_tmp[0], non_zero_tmp[1])\n",
    "        for (row_n, col_n) in non_zero:\n",
    "            self.terms_matrix[row_n, col_n] = self.terms_matrix[row_n, col_n] * self.idf_m[row_n]\n",
    "\n",
    "\n",
    "    def calculate_probability_normed(self, key_words):\n",
    "        q_vec = sparse.csr_matrix(self.words_frequency(key_words).multiply(self.idf_m))\n",
    "        \n",
    "        q_norm = sparse.linalg.norm(q_vec)\n",
    "        q_vec = q_vec / q_norm\n",
    "        \n",
    "        return sparse.csr_matrix.dot(q_vec, self.terms_matrix)\n",
    "\n",
    "    \n",
    "    def calculate_probability_svd(self, key_words):        \n",
    "        q_vec = sparse.csr_matrix(self.words_frequency(key_words).multiply(self.idf_m))\n",
    "        q_norm = sparse.linalg.norm(q_vec)\n",
    "        q_vec = q_vec / q_norm\n",
    "        \n",
    "        to_return = sparse.csr_matrix(q_vec).dot(self.u).dot(self.dvt)\n",
    "        return to_return\n",
    "        \n",
    "\n",
    "    def find_n_articles(self, key_words, n, mode=NORMED):\n",
    "        if self.terms_matrix is None:\n",
    "            raise(\"Terms Matrix not calculated\")\n",
    "        \n",
    "        if mode == NORMED:\n",
    "            probs = self.calculate_probability_normed(key_words)\n",
    "        elif mode == SVD:\n",
    "            if self.u is None or self.dvt is None:\n",
    "                raise(\"SVD not computed\")\n",
    "            probs = self.calculate_probability_svd(key_words)\n",
    "            \n",
    "        results = [(self.filenames[i], probs[0, i]) for i in probs.nonzero()[1]]\n",
    "        return heapq.nlargest(n, results, key=lambda t: t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words quantity: 7553\n",
      "__________________________\n",
      "Terms matrix computing...\n",
      "Done.\n",
      "__________________________\n",
      "IDF formatting...\n",
      "Done.\n",
      "__________________________\n",
      "Terms matrix normalizing...\n",
      "Done.\n",
      "__________________________\n",
      "Computing SVD decomposition...\n",
      "Done.\n",
      "__________________________\n",
      "Computing D @ V.T...\n",
      "Done.\n",
      "__________________________\n",
      "Preparing D @ V.T to give normalized Ak matrix...\n",
      "Done.\n",
      "__________________________\n"
     ]
    }
   ],
   "source": [
    "engine = SearchEngine(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7553\n",
      "[('resources/articles/Temblor_Formation.txt', 0.026502934831961703), ('resources/articles/International_Society_for_the_Interdisciplinary_Study_of_Symmetry.txt', 0.023779653388625897), ('resources/articles/New_Somerset,_Ohio.txt', 0.023535372807167), ('resources/articles/George_Jensen.txt', 0.022959752738068624), ('resources/articles/Klocks_Crossing,_Ohio.txt', 0.020666935975017897)]\n",
      "_________________________________________\n",
      "[('resources/articles/International_Society_for_the_Interdisciplinary_Study_of_Symmetry.txt', 0.07489663175616708), ('resources/articles/Tallassee,_Tennessee.txt', 0.04426693061736937), ('resources/articles/Pyrrha_of_Thessaly.txt', 0.03793645541795793), ('resources/articles/New_Somerset,_Ohio.txt', 0.031107638506970993), ('resources/articles/Alan_Webster_Neill.txt', 0.01815647807497398)]\n"
     ]
    }
   ],
   "source": [
    "engine = SearchEngine()\n",
    "print(engine.find_n_articles([\"research\", \"water\"], 5, mode=SVD))\n",
    "\n",
    "print(\"_________________________________________\")\n",
    "\n",
    "print(engine.find_n_articles([\"research\", \"water\"], 5, mode=NORMED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
